#Gradient descent
class GradientDescent:
    def __init__(self, lr=0.01, epochs=1000, tol=1e-3):
        self.learning_rate = lr
        self.epochs = epochs
        self.tolerance = tol
        self.weights = None
        self.bias = None

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

    def mean_squared_error(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.random.randn(n_features)
        self.bias = np.random.randn()

        for epoch in range(self.epochs):
            y_pred = self.predict(X)
            error = y_pred - y
            gradient_weights = np.dot(X.T, error) / n_samples
            gradient_bias = np.mean(error)

            self.weights -= self.learning_rate * gradient_weights
            self.bias -= self.learning_rate * gradient_bias

            if epoch % 100 == 0:
                loss = self.mean_squared_error(y, y_pred)
                print(f"Epoch {epoch}: Loss {loss}")

            if np.linalg.norm(gradient_weights) < self.tolerance:
                print("Convergence reached.")
                break

        return self.weights, self.bias

df = pd.DataFrame(data)

X = df[['Open', 'High', 'Low', 'Volume']].values
y = df['Close'].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

gradient_descent_model = GradientDescent(lr=0.01, epochs=1000, tol=1e-4)
gradient_descent_model.fit(X_scaled, y)

predictions = gradient_descent_model.predict(X_scaled)

results_df = pd.DataFrame({
    'Original Close': df['Close'][:20].tolist(),
    'Predicted Close': predictions[:20]
})
print(results_df)




#Stochastic gradient descent
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

data = pd.read_csv("/content/nvidia_stock_prices.csv")

import numpy as np

class SGD:
    def __init__(self, lr=0.01, epochs=1000, batch_size=32, tol=1e-3):
        self.learning_rate = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.tolerance = tol
        self.weights = None
        self.bias = None

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

    def mean_squared_error(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)

    def gradient(self, X_batch, y_batch):
        y_pred = self.predict(X_batch)
        error = y_pred - y_batch
        gradient_weights = np.dot(X_batch.T, error) / X_batch.shape[0]
        gradient_bias = np.mean(error)
        return gradient_weights, gradient_bias

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.random.randn(n_features)
        self.bias = np.random.randn()

        for epoch in range(self.epochs):
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            for i in range(0, n_samples, self.batch_size):
                X_batch = X_shuffled[i:i+self.batch_size]
                y_batch = y_shuffled[i:i+self.batch_size]

                gradient_weights, gradient_bias = self.gradient(X_batch, y_batch)
                self.weights -= self.learning_rate * gradient_weights
                self.bias -= self.learning_rate * gradient_bias

            if epoch % 100 == 0:
                y_pred = self.predict(X)
                loss = self.mean_squared_error(y, y_pred)
                print(f"Epoch {epoch}: Loss {loss}")

            if np.linalg.norm(gradient_weights) < self.tolerance:
                print("Convergence reached.")
                break

df = pd.DataFrame(data)

X = df[['Open', 'High', 'Low', 'Volume']].values
y = df['Close'].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
sgd_model = SGD(lr=0.001, epochs=1000, batch_size=5, tol=1e-4)
sgd_model.fit(X_scaled, y)

predictions = sgd_model.predict(X_scaled)

results_df = pd.DataFrame({
    'Original Close': df['Close'][:20].tolist(),
    'Predicted Close': predictions[:20]
})
print(results_df)
