import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

df = pd.read_csv(amazon_reviews.csv)

# Display first few rows
print(df.head())

# Step 2: Preprocess the data
# Remove missing values
df.dropna(subset=['text', 'label'], inplace=True)

# Map labels to integer (if not already)
df['label'] = df['label'].apply(lambda x: 1 if x == 'positive' else 0)

# Text Preprocessing
reviews = df['text'].values
labels = df['label'].values

# Split the dataset into training and testing data
x_train, x_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)

# Step 3: Tokenize the text data
vocab_size = 10000  # Limit the vocabulary to 10,000 most frequent words
max_length = 200  # Limit the length of each review to 200 words

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(x_train)

# Convert text data to sequences of integers (word indices)
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

# Pad sequences to ensure all inputs have the same length
x_train_pad = pad_sequences(x_train_seq, maxlen=max_length)
x_test_pad = pad_sequences(x_test_seq, maxlen=max_length)

# Step 4: Build the Sentiment Analysis Model using RNN (GRU/SimpleRNN)
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),
    SimpleRNN(128, activation='tanh', return_sequences=False),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))

# Step 6: Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test_pad, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 7: Making Predictions on New Reviews

# Example review (you can replace this with any Amazon review)
sample_review = "I love this product! It works as described and exceeded my expectations."

encoded_review = tokenizer.texts_to_sequences([sample_review.lower()])
padded_review = pad_sequences(encoded_review, maxlen=max_length)

prediction = model.predict(padded_review)

sentiment = "Positive" if prediction[0] > 0.5 else "Negative"
print(f"Predicted Sentiment: {sentiment}")

